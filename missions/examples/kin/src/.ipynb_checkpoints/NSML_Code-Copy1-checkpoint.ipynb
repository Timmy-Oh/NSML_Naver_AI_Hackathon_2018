{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../starter/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../starter/main.py\n",
    "# -*- coding: utf-8 -*-\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "### custom\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "### NSML\n",
    "import nsml\n",
    "from nsml import DATASET_PATH, HAS_DATASET, IS_ON_NSML\n",
    "from dataset import MovieReviewDataset, preprocess\n",
    "\n",
    "# DONOTCHANGE: They are reserved for nsml\n",
    "# This is for nsml leaderboard\n",
    "def bind_model(sess, config):\n",
    "    # 학습한 모델을 저장하는 함수입니다.\n",
    "    def save(dir_name, *args):\n",
    "        # directory\n",
    "        os.makedirs(dir_name, exist_ok=True)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, os.path.join(dir_name, 'model'))\n",
    "#         print(\"{} is saved on nsml\".format(os.path.join(dir_name, 'model')))\n",
    "#         with open(os.path.join(dir_name, 'dataset.pkl'), 'wb') as f:\n",
    "#             pkl.dump(dataset, f)\n",
    "#             print(\"dataset is saved on nsml\")\n",
    "\n",
    "    # 저장한 모델을 불러올 수 있는 함수입니다.\n",
    "    def load(dir_name, *args):\n",
    "        saver = tf.train.Saver()\n",
    "        # find checkpoint\n",
    "        ckpt = tf.train.get_checkpoint_state(dir_name)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            checkpoint = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            saver.restore(sess, os.path.join(dir_name, checkpoint))\n",
    "#             with open(os.path.join(dir_name, 'dataset.pkl'), 'rb') as f:\n",
    "#                 dataset = pkl.load(f)\n",
    "        else:\n",
    "            raise NotImplemented('No checkpoint!')\n",
    "        print('Model loaded')\n",
    "\n",
    "    def infer(raw_data, **kwargs):\n",
    "        \"\"\"\n",
    "        :param raw_data: raw input (여기서는 문자열)을 입력받습니다\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "#         preprocessed_data = dataset.preprocess(raw_data)\n",
    "#         embedded_data = dataset.test_embedding(raw_data)\n",
    "        embedded_data = preprocess(raw_data, config.strmaxlen)\n",
    "    \n",
    "        # 저장한 모델에 입력값을 넣고 prediction 결과를 리턴받습니다\n",
    "        pred = sess.run(output, feed_dict={x: embedded_data})\n",
    "        point = pred.squeeze(axis=1).tolist()\n",
    "        print(np.shape(point))\n",
    "        # DONOTCHANGE: They are reserved for nsml\n",
    "        # 리턴 결과는 [(확률, 0 or 1)] 의 형태로 보내야만 리더보드에 올릴 수 있습니다. 리더보드 결과에 확률의 값은 영향을 미치지 않습니다\n",
    "        return list(zip(np.zeros(len(point)), point))\n",
    "    \n",
    "    # DONOTCHANGE: They are reserved for nsml\n",
    "    # nsml에서 지정한 함수에 접근할 수 있도록 하는 함수입니다.\n",
    "    nsml.bind(save=save, load=load, infer=infer)\n",
    "\n",
    "def _batch_loader(iterable, n=1):\n",
    "    \"\"\"\n",
    "    데이터를 배치 사이즈만큼 잘라서 보내주는 함수입니다. PyTorch의 DataLoader와 같은 역할을 합니다\n",
    "\n",
    "    :param iterable: 데이터 list, 혹은 다른 포맷\n",
    "    :param n: 배치 사이즈\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    length = len(iterable)\n",
    "    for n_idx in range(0, length, n):\n",
    "        yield iterable[n_idx:min(n_idx + n, length)]\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "        \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args = argparse.ArgumentParser()\n",
    "    # DONOTCHANGE: They are reserved for nsml\n",
    "    args.add_argument('--mode', type=str, default='train')\n",
    "    args.add_argument('--pause', type=int, default=0)\n",
    "    args.add_argument('--iteration', type=str, default='0')\n",
    "\n",
    "    # User options\n",
    "    args.add_argument('--output', type=int, default=1)\n",
    "    args.add_argument('--epochs', type=int, default=5)\n",
    "    args.add_argument('--strmaxlen', type=int, default=400)\n",
    "    \n",
    "    args.add_argument('--maxlen', type=int, default=400)\n",
    "    args.add_argument('--cell_size', type=int, default=40)\n",
    "    args.add_argument('--embed_size', type=int, default=300)\n",
    "    args.add_argument('--prob_dropout', type=float, default=0.4)\n",
    "    args.add_argument('--max_features', type=int, default=431)\n",
    "    args.add_argument('--batch_size', type=int, default=256)\n",
    "    \n",
    "    config = args.parse_args()\n",
    " \n",
    "\n",
    "    if not HAS_DATASET and not IS_ON_NSML:  # It is not running on nsml\n",
    "        DATASET_PATH = '../sample_data/movie_review/'\n",
    "\n",
    "     # 모델의 specification\n",
    "    input_size = config.embed_size*config.maxlen\n",
    "    output_size = 1\n",
    "    hidden_layer_size = 200\n",
    "    learning_rate = 0.001\n",
    "    character_size = 251\n",
    "    emb_train = True\n",
    "\n",
    "    x = tf.placeholder(tf.int32, [None, config.maxlen])\n",
    "    y_ = tf.placeholder(tf.float32, [None, output_size])\n",
    "    \n",
    "    # 임베딩\n",
    "    word_embedding = tf.get_variable('word_embedding', [config.max_features, config.embed_size])\n",
    "    embedded = tf.nn.embedding_lookup(word_embedding, x)\n",
    "\n",
    "    # 첫 번째 레이어\n",
    "    first_layer_weight = weight_variable([input_size, hidden_layer_size])\n",
    "    first_layer_bias = bias_variable([hidden_layer_size])\n",
    "    hidden_layer = tf.matmul(tf.reshape(embedded, (-1, input_size)), first_layer_weight) + first_layer_bias\n",
    "\n",
    "    # 두 번째 (아웃풋) 레이어\n",
    "    second_layer_weight = weight_variable([hidden_layer_size, output_size])\n",
    "    second_layer_bias = bias_variable([output_size])\n",
    "    output = tf.matmul(hidden_layer, second_layer_weight) + second_layer_bias\n",
    "#     output_sigmoid = tf.sigmoid(output)\n",
    "\n",
    "    # loss와 optimizer\n",
    "    loss_mse = tf.losses.mean_squared_error(y_, output)\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss_mse)\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "    tf.global_variables_initializer().run()\n",
    "        \n",
    "    # DONOTCHANGE: Reserved for nsml\n",
    "    dataset = MovieReviewDataset(DATASET_PATH, config.strmaxlen)\n",
    "#     dataset.dset_regex_morph(morph=True)\n",
    "#     dataset.dset_embedding()\n",
    "    bind_model(sess=sess, config=config)\n",
    "\n",
    "   \n",
    "    # DONOTCHANGE: Reserved for nsml\n",
    "    if config.pause:\n",
    "        nsml.paused(scope=locals())\n",
    "    \n",
    "    if config.mode == 'train':\n",
    "#         dataset = MovieReviewDataset(DATASET_PATH, config.strmaxlen)\n",
    "#         dataset.dset_regex_morph(morph=True)\n",
    "#         dataset.dset_embedding()\n",
    "        \n",
    "\n",
    "        \n",
    "        ### csutom\n",
    "#         dataset.dset_regex_morph(morph=True)\n",
    "#         emb_path=\"D:/onedrive/code/ipython/wordvec/pre_trained/fasttext/ko/wiki.ko.vec\"\n",
    "#         dataset.load_emb_model(embedding_path=emb_path)\n",
    "#         dataset.dset_embedding(dataset.emb_model)\n",
    "#         dataset.dset_embedding()\n",
    "        \n",
    "        dataset_len = len(dataset)\n",
    "        one_batch_size = dataset_len//config.batch_size\n",
    "        if dataset_len % config.batch_size != 0:\n",
    "            one_batch_size += 1\n",
    "        # epoch마다 학습을 수행합니다.\n",
    "        for epoch in range(config.epochs):\n",
    "            avg_loss = 0.0\n",
    "            for i, (data, labels) in enumerate(_batch_loader(dataset, config.batch_size)):\n",
    "                _, loss = sess.run([train_step, loss_mse],\n",
    "                                   feed_dict={x: data, y_: labels})\n",
    "#                 print('Batch : ', i + 1, '/', one_batch_size,\n",
    "#                       ', BCE in this minibatch: ', float(loss))\n",
    "                avg_loss += float(loss)\n",
    "            print('epoch:', epoch, ' train_loss:', float(avg_loss/one_batch_size))\n",
    "            nsml.report(summary=True, scope=locals(), epoch=epoch, epoch_total=config.epochs,\n",
    "                        train__loss=float(avg_loss/one_batch_size), step=epoch)\n",
    "            # DONOTCHANGE (You can decide how often you want to save the model)\n",
    "            nsml.save(epoch)\n",
    "\n",
    "    # 로컬 테스트 모드일때 사용합니다\n",
    "    # 결과가 아래와 같이 나온다면, nsml submit을 통해서 제출할 수 있습니다.\n",
    "    # [(0.3, 0), (0.7, 1), ... ]\n",
    "    elif config.mode == 'test_local':\n",
    "        with open(os.path.join(DATASET_PATH, 'test/test_data'), 'rt', encoding='utf-8') as f:\n",
    "            reviews = f.readlines()\n",
    "        res = []\n",
    "        for batch in _batch_loader(reviews, config.batch_size):\n",
    "            temp_res = nsml.infer(batch)\n",
    "            res += temp_res\n",
    "#         res = nsml.infer(reviews)\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  train_loss: 118.17220306396484\n",
      "epoch: 1  train_loss: 4420.47900390625\n",
      "epoch: 2  train_loss: 319.73101806640625\n",
      "epoch: 3  train_loss: 829.7860107421875\n",
      "epoch: 4  train_loss: 2231.032958984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-03 21:16:15.717569: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n",
      "2018-04-03 21:16:16.031441: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1212] Found device 0 with properties: \n",
      "name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\n",
      "pciBusID: 0000:01:00.0\n",
      "totalMemory: 11.00GiB freeMemory: 9.09GiB\n",
      "2018-04-03 21:16:16.031763: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1312] Adding visible gpu devices: 0\n",
      "2018-04-03 21:16:16.648086: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8806 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "!python ../starter/main.py --mode train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87,)\n",
      "[(0.0, -1.0029360055923462), (0.0, -0.5922660827636719), (0.0, -0.5258321762084961), (0.0, -0.46770694851875305), (0.0, -0.9856932163238525), (0.0, 0.17409417033195496), (0.0, -1.0555821657180786), (0.0, -0.9003530740737915), (0.0, -1.25115966796875), (0.0, -0.8233543634414673), (0.0, -0.454477459192276), (0.0, -1.193847417831421), (0.0, -0.004324875771999359), (0.0, -0.9096697568893433), (0.0, -0.23301494121551514), (0.0, -0.6798992156982422), (0.0, -0.9385336637496948), (0.0, 0.20161789655685425), (0.0, -0.6412708759307861), (0.0, -1.9880775213241577), (0.0, -0.6272163391113281), (0.0, -0.5542000532150269), (0.0, -0.5005652904510498), (0.0, -0.42831066250801086), (0.0, 0.4964554011821747), (0.0, -0.6084188222885132), (0.0, -1.38687264919281), (0.0, -0.5787463188171387), (0.0, -2.1129531860351562), (0.0, -0.3341926038265228), (0.0, -1.610254168510437), (0.0, -1.316604733467102), (0.0, 0.0038783326745033264), (0.0, 0.6582373380661011), (0.0, -1.1776357889175415), (0.0, -0.21557018160820007), (0.0, -0.4540349543094635), (0.0, -0.8899532556533813), (0.0, -1.1772528886795044), (0.0, -0.589270830154419), (0.0, -1.2657780647277832), (0.0, -1.0944963693618774), (0.0, -0.269546777009964), (0.0, -0.7613799571990967), (0.0, -0.8739781379699707), (0.0, -0.8732295036315918), (0.0, -0.32393908500671387), (0.0, -0.4817609488964081), (0.0, -0.5272475481033325), (0.0, -1.1202242374420166), (0.0, 0.1692580282688141), (0.0, -0.8525235056877136), (0.0, -0.7941698431968689), (0.0, -0.9751657247543335), (0.0, -0.19394001364707947), (0.0, -0.7400272488594055), (0.0, -0.5845773816108704), (0.0, -0.6199964284896851), (0.0, -0.36087480187416077), (0.0, -0.0714925155043602), (0.0, -0.615979790687561), (0.0, -0.22929587960243225), (0.0, -0.5281720161437988), (0.0, -2.1663765907287598), (0.0, 0.24768027663230896), (0.0, -0.5579814910888672), (0.0, 0.034176625311374664), (0.0, 0.29490241408348083), (0.0, -0.42823806405067444), (0.0, -0.9151004552841187), (0.0, -0.418430894613266), (0.0, 0.252248078584671), (0.0, -0.9105356931686401), (0.0, -0.7554827928543091), (0.0, -0.11407818645238876), (0.0, 0.3962583839893341), (0.0, -0.14480659365653992), (0.0, -0.37320223450660706), (0.0, 0.2893032133579254), (0.0, -0.5939229130744934), (0.0, -1.886802077293396), (0.0, -1.1097495555877686), (0.0, -0.5962474346160889), (0.0, -1.1573292016983032), (0.0, -0.6542625427246094), (0.0, 0.30222973227500916), (0.0, -0.6819044351577759)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-03 21:16:20.067405: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n",
      "2018-04-03 21:16:20.373957: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1212] Found device 0 with properties: \n",
      "name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\n",
      "pciBusID: 0000:01:00.0\n",
      "totalMemory: 11.00GiB freeMemory: 9.09GiB\n",
      "2018-04-03 21:16:20.374311: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1312] Adding visible gpu devices: 0\n",
      "2018-04-03 21:16:20.953064: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8806 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "!python ../starter/main.py --mode test_local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch :  1 / 1 , MSE in this minibatch:  19.29180335998535\n",
      "epoch: 0  train_loss: 19.29180335998535\n",
      "Batch :  1 / 1 , MSE in this minibatch:  7.607476711273193\n",
      "epoch: 1  train_loss: 7.607476711273193\n",
      "Batch :  1 / 1 , MSE in this minibatch:  7.607476711273193\n",
      "epoch: 2  train_loss: 7.607476711273193\n",
      "Batch :  1 / 1 , MSE in this minibatch:  7.607476711273193\n",
      "epoch: 3  train_loss: 7.607476711273193\n",
      "Batch :  1 / 1 , MSE in this minibatch:  7.607476711273193\n",
      "epoch: 4  train_loss: 7.607476711273193\n",
      "Batch :  1 / 1 , MSE in this minibatch:  7.607476711273193\n",
      "epoch: 5  train_loss: 7.607476711273193\n",
      "Batch :  1 / 1 , MSE in this minibatch:  7.607476711273193\n",
      "epoch: 6  train_loss: 7.607476711273193\n",
      "Batch :  1 / 1 , MSE in this minibatch:  7.607476711273193\n",
      "epoch: 7  train_loss: 7.607476711273193\n",
      "Batch :  1 / 1 , MSE in this minibatch:  7.607476711273193\n",
      "epoch: 8  train_loss: 7.607476711273193\n",
      "Batch :  1 / 1 , MSE in this minibatch:  7.607476711273193\n",
      "epoch: 9  train_loss: 7.607476711273193\n"
     ]
    }
   ],
   "source": [
    "!python ../example_back/main.py --mode train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [(0.0, 5.507122039794922), (0.0, 5.477208137512207), (0.0, 5.081559658050537), (0.0, 5.745182037353516), (0.0, 6.0997090339660645), (0.0, 5.659692287445068), (0.0, 5.920278072357178), (0.0, 6.350973606109619), (0.0, 5.890348434448242), (0.0, 5.882244110107422), (0.0, 5.47745418548584), (0.0, 5.883562088012695), (0.0, 5.641011714935303), (0.0, 4.899208068847656), (0.0, 5.4042487144470215), (0.0, 5.44482421875), (0.0, 5.365687847137451), (0.0, 5.508814334869385), (0.0, 5.610958576202393), (0.0, 6.146960735321045), (0.0, 5.443153381347656), (0.0, 5.225772857666016), (0.0, 5.609511852264404), (0.0, 5.245474338531494), (0.0, 5.728208065032959), (0.0, 5.431302547454834), (0.0, 5.909133434295654), (0.0, 5.599045753479004), (0.0, 5.0513811111450195), (0.0, 5.178066253662109), (0.0, 5.482705593109131), (0.0, 5.959336280822754), (0.0, 5.443247318267822), (0.0, 5.457961559295654), (0.0, 5.3761372566223145), (0.0, 5.327464580535889), (0.0, 4.982173919677734), (0.0, 4.972344398498535), (0.0, 5.525853157043457), (0.0, 5.402675628662109), (0.0, 5.452371597290039), (0.0, 5.296295166015625), (0.0, 5.418962001800537), (0.0, 5.954452991485596), (0.0, 5.593425273895264), (0.0, 5.549355983734131), (0.0, 5.962398529052734), (0.0, 5.473674297332764), (0.0, 5.136119365692139), (0.0, 5.7232985496521), (0.0, 5.20017671585083), (0.0, 4.964993476867676), (0.0, 5.232919692993164), (0.0, 5.040822505950928), (0.0, 5.335686206817627), (0.0, 5.714105606079102), (0.0, 5.286571502685547), (0.0, 5.32389497756958), (0.0, 6.3306097984313965), (0.0, 5.624547958374023), (0.0, 5.531543254852295), (0.0, 5.307179927825928), (0.0, 5.559380531311035), (0.0, 5.755814075469971), (0.0, 5.190467357635498), (0.0, 5.5357513427734375), (0.0, 5.294408798217773), (0.0, 5.54160213470459), (0.0, 5.4538254737854), (0.0, 5.685484409332275), (0.0, 5.525065898895264), (0.0, 5.39377498626709), (0.0, 5.509948253631592), (0.0, 5.330820083618164), (0.0, 5.601436138153076), (0.0, 5.052368640899658), (0.0, 5.631039142608643), (0.0, 6.053228378295898), (0.0, 5.571783542633057), (0.0, 5.3911824226379395), (0.0, 5.865889549255371), (0.0, 5.761862754821777), (0.0, 5.490329742431641), (0.0, 5.48750638961792), (0.0, 4.960508346557617), (0.0, 5.481670379638672), (0.0, 5.939573764801025), (0.0, 5.222892761230469), (0.0, 5.459935665130615), (0.0, 5.423129081726074), (0.0, 5.0208635330200195), (0.0, 5.444896221160889), (0.0, 5.56053352355957), (0.0, 5.544216156005859), (0.0, 5.691243648529053), (0.0, 5.1534810066223145), (0.0, 5.399294376373291), (0.0, 5.101796627044678), (0.0, 5.719296455383301), (0.0, 5.345041751861572), (0.0, 5.30886697769165), (0.0, 5.01315450668335), (0.0, 5.454311847686768), (0.0, 5.488786220550537), (0.0, 5.514221668243408), (0.0, 5.336738109588623), (0.0, 5.635279655456543)]\n",
    "tmp2 = [(0.0, 0.17151004076004028), (0.0, 0.3102037310600281), (0.0, 0.8511660695075989), (0.0, 0.11610407382249832), (0.0, 0.37960124015808105), (0.0, 0.49174389243125916), (0.0, 1.0346283912658691), (0.0, 0.9915686845779419), (0.0, 0.08578527718782425), (0.0, 1.8068877458572388), (0.0, 0.8327751755714417), (0.0, 0.650511622428894), (0.0, 0.41550737619400024), (0.0, 0.6187836527824402), (0.0, -0.13442781567573547), (0.0, 1.1096271276474), (0.0, 0.1299227476119995), (0.0, 0.7282607555389404), (0.0, 0.5944207906723022), (0.0, 1.1815965175628662), (0.0, -0.919247031211853), (0.0, 1.716301679611206), (0.0, 1.3576778173446655), (0.0, 0.6849305033683777), (0.0, 0.3809773325920105), (0.0, 0.7658463716506958), (0.0, 1.1667943000793457), (0.0, 1.3709861040115356), (0.0, 0.9598567485809326), (0.0, 0.8689199686050415), (0.0, 0.26576030254364014), (0.0, 0.5343411564826965), (0.0, 0.5739362835884094), (0.0, 0.2807602882385254), (0.0, 0.6746336817741394), (0.0, 0.5748001933097839), (0.0, 0.7115890383720398), (0.0, 0.9866825938224792), (0.0, 1.0107001066207886), (0.0, 0.45365607738494873), (0.0, 1.1045211553573608), (0.0, 0.02827710658311844), (0.0, 0.5486428141593933), (0.0, 0.2230035364627838), (0.0, 1.0092824697494507), (0.0, 0.8335793018341064), (0.0, 0.7652144432067871), (0.0, 0.8846083879470825), (0.0, 0.25722795724868774), (0.0, 0.023475296795368195), (0.0, 1.0517147779464722), (0.0, 0.8199621438980103), (0.0, 0.17542585730552673), (0.0, 0.9163600206375122), (0.0, 0.5567643046379089), (0.0, 1.1359342336654663), (0.0, 0.21362942457199097), (0.0, 0.09889668971300125), (0.0, 0.4078075587749481), (0.0, 0.7591463327407837), (0.0, 0.4542941749095917), (0.0, 0.39212343096733093), (0.0, 0.42554154992103577), (0.0, 0.8650057315826416), (0.0, 1.1026344299316406), (0.0, 1.3823593854904175), (0.0, 0.6251580715179443), (0.0, 0.38394099473953247), (0.0, 0.5180602669715881), (0.0, -0.3579539954662323), (0.0, 0.12424284964799881), (0.0, 0.8763779401779175), (0.0, 0.35700929164886475), (0.0, 0.5003722310066223), (0.0, 0.11724204570055008), (0.0, 0.5098779201507568), (0.0, 0.5466137528419495), (0.0, 0.28781092166900635), (0.0, -0.4003223478794098), (0.0, 1.2976845502853394), (0.0, 0.3018210232257843), (0.0, 2.049747943878174), (0.0, 0.46604761481285095), (0.0, 0.7953343987464905), (0.0, 0.6959224939346313), (0.0, 1.383487582206726), (0.0, -0.5519022941589355)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107, 2)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [5.849471092224121, 4.6791486740112305, 5.030023574829102, 4.490840435028076, 5.456708908081055, 5.024709701538086, 5.023854732513428, 5.876331329345703, 4.961490631103516, 5.1269001960754395, 4.586499214172363, 5.601384162902832, 5.2715301513671875, 5.52301549911499, 5.54022216796875, 5.463802337646484, 5.524343013763428, 5.513424873352051, 5.404085159301758, 5.085888862609863, 5.18102502822876, 4.819967269897461, 4.992971897125244, 5.379040241241455, 5.407474994659424, 5.209883213043213, 5.179623126983643, 4.9488115310668945, 5.008663177490234, 4.976510047912598, 4.887200355529785, 5.230838298797607, 4.875622272491455, 5.012160778045654, 4.959322929382324, 4.795332908630371, 5.235421657562256, 5.2926201820373535, 5.035041809082031, 5.089512825012207, 4.789463043212891, 5.048935413360596, 4.758528709411621, 5.34534215927124, 5.352360725402832, 4.331325531005859, 4.898258209228516, 5.234723091125488, 5.124054908752441, 5.020263195037842, 4.730261325836182, 4.833620071411133, 5.053152561187744, 5.565540790557861, 5.36036491394043, 5.275566577911377, 4.7412919998168945, 4.223257541656494, 4.780771255493164, 5.20416784286499, 4.944936275482178, 5.273554801940918, 4.920029640197754, 5.362622261047363, 4.745726585388184, 5.217639923095703, 5.082106113433838, 4.781542778015137, 4.717349052429199, 5.029264450073242, 5.311281681060791, 4.725761413574219, 5.068767070770264, 5.286892414093018, 4.738636493682861, 4.955305099487305, 4.970938682556152, 5.201949596405029, 4.973832130432129, 5.466550350189209, 4.975394248962402, 4.786642074584961, 5.051764965057373, 4.621184349060059, 5.357519149780273, 4.947997093200684, 4.799294471740723, 4.8324103355407715, 5.573225498199463, 4.858403205871582, 5.014888286590576, 5.209362030029297, 4.926170825958252, 4.9326958656311035, 4.597771167755127, 4.772767066955566, 4.680015563964844, 4.746307373046875, 5.023454666137695, 4.613122940063477, 5.03616189956665, 4.5199127197265625, 4.76422119140625, 4.486040115356445, 4.875519275665283, 4.937893867492676, 5.213864803314209]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.849471092224121, 4.6791486740112305, 5.030023574829102, 4.490840435028076, 5.456708908081055, 5.024709701538086, 5.023854732513428, 5.876331329345703, 4.961490631103516, 5.1269001960754395, 4.586499214172363, 5.601384162902832, 5.2715301513671875, 5.52301549911499, 5.54022216796875, 5.463802337646484, 5.524343013763428, 5.513424873352051, 5.404085159301758, 5.085888862609863, 5.18102502822876, 4.819967269897461, 4.992971897125244, 5.379040241241455, 5.407474994659424, 5.209883213043213, 5.179623126983643, 4.9488115310668945, 5.008663177490234, 4.976510047912598, 4.887200355529785, 5.230838298797607, 4.875622272491455, 5.012160778045654, 4.959322929382324, 4.795332908630371, 5.235421657562256, 5.2926201820373535, 5.035041809082031, 5.089512825012207, 4.789463043212891, 5.048935413360596, 4.758528709411621, 5.34534215927124, 5.352360725402832, 4.331325531005859, 4.898258209228516, 5.234723091125488, 5.124054908752441, 5.020263195037842, 4.730261325836182, 4.833620071411133, 5.053152561187744, 5.565540790557861, 5.36036491394043, 5.275566577911377, 4.7412919998168945, 4.223257541656494, 4.780771255493164, 5.20416784286499, 4.944936275482178, 5.273554801940918, 4.920029640197754, 5.362622261047363, 4.745726585388184, 5.217639923095703, 5.082106113433838, 4.781542778015137, 4.717349052429199, 5.029264450073242, 5.311281681060791, 4.725761413574219, 5.068767070770264, 5.286892414093018, 4.738636493682861, 4.955305099487305, 4.970938682556152, 5.201949596405029, 4.973832130432129, 5.466550350189209, 4.975394248962402, 4.786642074584961, 5.051764965057373, 4.621184349060059, 5.357519149780273, 4.947997093200684, 4.799294471740723, 4.8324103355407715, 5.573225498199463, 4.858403205871582, 5.014888286590576, 5.209362030029297, 4.926170825958252, 4.9326958656311035, 4.597771167755127, 4.772767066955566, 4.680015563964844, 4.746307373046875, 5.023454666137695, 4.613122940063477, 5.03616189956665, 4.5199127197265625, 4.76422119140625, 4.486040115356445, 4.875519275665283, 4.937893867492676, 5.213864803314209]\n"
     ]
    }
   ],
   "source": [
    "!python ../example_back/main.py --mode test_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../starter/Dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../starter/Dataset.py\n",
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "\n",
    "#===============keras ==============\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "#===============morphnizer ============\n",
    "# from konlpy.tag import Twitter\n",
    "# twt = Twitter()\n",
    "\n",
    "class MovieReviewDataset():\n",
    "    \"\"\"\n",
    "    영화리뷰 데이터를 읽어서, tuple (데이터, 레이블)의 형태로 리턴하는 파이썬 오브젝트 입니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_path: str, max_length: int):\n",
    "        \"\"\"\n",
    "        initializer\n",
    "        :param dataset_path: 데이터셋 root path\n",
    "        :param max_length: 문자열의 최대 길이\n",
    "        \"\"\"\n",
    "\n",
    "        # 데이터, 레이블 각각의 경로\n",
    "        data_review = os.path.join(dataset_path, 'train', 'train_data')\n",
    "        data_label = os.path.join(dataset_path, 'train', 'train_label')\n",
    "\n",
    "        # 영화리뷰 데이터를 읽고 preprocess까지 진행합니다\n",
    "        with open(data_review, 'rt', encoding='utf-8') as f:\n",
    "            self.reviews = f.readlines()\n",
    "        # 영화리뷰 레이블을 읽고 preprocess까지 진행합니다.\n",
    "        with open(data_label) as f:\n",
    "            self.labels = [np.float32(x) for x in f.readlines()]\n",
    "            \n",
    "        self.dset = pd.DataFrame(data=np.array([self.reviews, self.labels]).T, columns=['reviews', 'labels'])\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "\n",
    "        :return: 전체 데이터의 수를 리턴합니다\n",
    "        \"\"\"\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "\n",
    "        :param idx: 필요한 데이터의 인덱스\n",
    "        :return: 인덱스에 맞는 데이터, 레이블 pair를 리턴합니다\n",
    "        \"\"\"\n",
    "        return self.reviews[idx], self.labels[idx]\n",
    "    \n",
    "    def export(self):\n",
    "        self.dset.to_csv(\"./export.csv\")\n",
    "        \n",
    "    def prin(self):\n",
    "        print(self.dset)   \n",
    "\n",
    "    def load_emb_model(self, embedding_path, encodings = \"utf-8\"):\n",
    "        def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "        self.emb_model = dict(get_coefs(*o.strip().split(\" \")) for o in codecs.open(embedding_path, \"r\", encodings ))\n",
    "    \n",
    "    def preprocess(self, raw_data, morph=True):\n",
    "        docs = []\n",
    "        for doc in raw_data:\n",
    "            doc = re.sub('[^\\w?!]', ' ', doc)\n",
    "            doc = re.sub('[\\s]+', ' ', doc)\n",
    "            doc = re.sub('[\\s]$|^[\\s]', '', doc)\n",
    "#             if morph:\n",
    "#                 docs.append(\" \".join(twt.morphs(doc)))\n",
    "#             else:\n",
    "#                 docs.append(doc)\n",
    "            docs.append(doc)\n",
    "        return docs\n",
    "    \n",
    "    def test_embedding(self, raw_data,\n",
    "#                        emb_model,\n",
    "                       embed_size = 300,\n",
    "                       max_features = 100000,\n",
    "                       maxlen = 20,\n",
    "                       oov_zero = True,\n",
    "                       truncating='pre'\n",
    "                      ):\n",
    "        \n",
    "\n",
    "        list_sentences = raw_data \n",
    "        list_tokenized = self.tokenizer.texts_to_sequences(list_sentences)\n",
    "        X = sequence.pad_sequences(list_tokenized, maxlen=maxlen, truncating=truncating)\n",
    "        return X\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def dset_regex_morph(self, morph=True):\n",
    "        docs = []\n",
    "        for doc in self.dset['reviews']:\n",
    "            doc = re.sub('[^\\w?!]', ' ', doc)\n",
    "            doc = re.sub('[\\s]+', ' ', doc)\n",
    "            doc = re.sub('[\\s]$|^[\\s]', '', doc)\n",
    "#             if morph:\n",
    "#                 docs.append(\" \".join(twt.morphs(doc)))\n",
    "#             else:\n",
    "#                 docs.append(doc)\n",
    "            docs.append(doc)\n",
    "        self.dset['reviews'] = docs\n",
    "    \n",
    "    def dset_embedding(self,\n",
    "#                        emb_model,\n",
    "                       embed_size = 300,\n",
    "                       max_features = 100000,\n",
    "                       maxlen = 20,\n",
    "                       oov_zero = True,\n",
    "                       truncating='pre'\n",
    "                      ):\n",
    "        \n",
    "        doc_column = \"reviews\"\n",
    "        list_classes = [\"labels\"]\n",
    "\n",
    "        list_sentences = self.dset[doc_column].fillna('UNK').values.tolist()\n",
    "        \n",
    "\n",
    "        self.tokenizer = text.Tokenizer(num_words =max_features)\n",
    "        self.tokenizer.fit_on_texts(list_sentences)\n",
    "\n",
    "        list_tokenized = self.tokenizer.texts_to_sequences(list_sentences)\n",
    "        \n",
    "        X = sequence.pad_sequences(list_tokenized, maxlen=maxlen, truncating=truncating)\n",
    "        Y = self.dset[list_classes].values\n",
    "        print(\"=== Data is preprocessed\")\n",
    "\n",
    "#         word_index = tokenizer.word_index\n",
    "#         nb_words = min(max_features, len(word_index))\n",
    "\n",
    "#         if oov_zero:\n",
    "#             embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "#         else:\n",
    "#             embedding_matrix = np.random.normal(0.001, 0.4, (nb_words, embed_size))\n",
    "\n",
    "#         for word, i in word_index.items():\n",
    "#             if i >= max_features: continue\n",
    "#             try:\n",
    "#                 embedding_vector = emb_model.get(word)\n",
    "#                 if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "#             except: \n",
    "#                 pass\n",
    "#         print(\"=== Embedding Matrix is loaded\")\n",
    "        \n",
    "        self.reviews = X\n",
    "        self.labels = Y\n",
    "#         self.emb_matrix = embedding_matrix\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
