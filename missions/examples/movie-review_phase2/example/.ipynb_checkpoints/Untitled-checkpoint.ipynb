{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_size = 80 \n",
    "cell_type_GRU = True\n",
    "maxlen = 180 \n",
    "max_features = 100000\n",
    "embed_size = 300\n",
    "prob_dropout = 0.2\n",
    "emb_train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.rand(100000,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen, ), name='input')\n",
    "\n",
    "##pre\n",
    "x1 = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable = emb_train)(inp)\n",
    "x1 = SpatialDropout1D(prob_dropout)(x1)\n",
    "\n",
    "if cell_type_GRU:\n",
    "    x1 = Bidirectional(CuDNNLSTM(cell_size, return_sequences=True))(x1)\n",
    "    x1 = Bidirectional(CuDNNGRU(cell_size, return_sequences=True))(x1)\n",
    "else :\n",
    "    x1 = Bidirectional(CuDNNLSTM(cell_size, return_sequences=True))(x1)\n",
    "    x1 = Bidirectional(CuDNNLSTM(cell_size, return_sequences=True))(x1)\n",
    "\n",
    "avg_pool1 = GlobalAveragePooling1D()(x1)\n",
    "max_pool1 = GlobalMaxPooling1D()(x1)\n",
    "\n",
    "##merge\n",
    "conc = concatenate([avg_pool1, max_pool1])\n",
    "outp = Dense(1)(conc)\n",
    "\n",
    "model = Model(inputs=inp, outputs=outp)\n",
    "\n",
    "# DONOTCHANGE: Reserved for nsml use\n",
    "bind_model(model, config)\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error', 'accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trial.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trial.py\n",
    "# -*- coding: utf-8 -*-\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "### custom\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "\n",
    "# keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input, concatenate, Flatten, add, multiply\n",
    "from keras.layers import CuDNNLSTM, CuDNNGRU, Bidirectional, Conv1D\n",
    "from keras.layers import Dropout, SpatialDropout1D, BatchNormalization, GlobalAveragePooling1D, GlobalMaxPooling1D, PReLU\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import K, Activation\n",
    "from keras.engine import Layer\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "# #===============keras ==============\n",
    "# from keras.preprocessing import text, sequence\n",
    "\n",
    "# #===============morphnizer ============\n",
    "# from konlpy.tag import Twitter\n",
    "\n",
    "### NSML\n",
    "# import nsml\n",
    "# from nsml import DATASET_PATH, HAS_DATASET, IS_ON_NSML\n",
    "from pipeline import MovieReviewDataset\n",
    "# from model import get_model_2rnn\n",
    "\n",
    "IS_ON_NSML = False\n",
    "HAS_DATASET = False\n",
    "\n",
    "# DONOTCHANGE: They are reserved for nsml\n",
    "# This is for nsml leaderboard\n",
    "\n",
    "def bind_model(model):\n",
    "    # 학습한 모델을 저장하는 함수입니다.\n",
    "    def save(filename, *args):\n",
    "        # directory\n",
    "        model.save(filename=filename)\n",
    "\n",
    "    # 저장한 모델을 불러올 수 있는 함수입니다.\n",
    "    def load(filename, *args):\n",
    "        model.load_weights(filename)\n",
    "    \n",
    "    def infer(raw_data, **kwargs):\n",
    "        \"\"\"\n",
    "        :param raw_data: raw input (여기서는 문자열)을 입력받습니다\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # dataset.py에서 작성한 preprocess 함수를 호출하여, 문자열을 벡터로 변환합니다\n",
    "        preprocessed_data = MovieReviewDataset(raw_data, 400)\n",
    "#         model.evaluate()\n",
    "\n",
    "        # 저장한 모델에 입력값을 넣고 prediction 결과를 리턴받습니다\n",
    "        output_prediction = model.predict(preprocessed_data)\n",
    "        point = output_prediction.data.squeeze(dim=1).tolist()\n",
    "        # DONOTCHANGE: They are reserved for nsml\n",
    "        # 리턴 결과는 [(confidence interval, 포인트)] 의 형태로 보내야만 리더보드에 올릴 수 있습니다. 리더보드 결과에 confidence interval의 값은 영향을 미치지 않습니다\n",
    "     \n",
    "    # DONOTCHANGE: They are reserved for nsml\n",
    "    # nsml에서 지정한 함수에 접근할 수 있도록 하는 함수입니다.\n",
    "    nsml.bind(save=save, load=load, infer=infer)\n",
    "    \n",
    "class Nsml_Callback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        bind_model(model=self.model)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args = argparse.ArgumentParser()\n",
    "    # DONOTCHANGE: They are reserved for nsml\n",
    "    args.add_argument('--mode', type=str, default='train')\n",
    "    args.add_argument('--pause', type=int, default=0)\n",
    "    args.add_argument('--iteration', type=str, default='0')\n",
    "\n",
    "    # User options\n",
    "    args.add_argument('--output', type=int, default=1)\n",
    "    args.add_argument('--epochs', type=int, default=10)\n",
    "    args.add_argument('--batch', type=int, default=2000)\n",
    "    args.add_argument('--strmaxlen', type=int, default=400)\n",
    "    \n",
    "    args.add_argument('--maxlen', type=int, default=20)\n",
    "    args.add_argument('--cell_size', type=int, default=40)\n",
    "    args.add_argument('--embed_size', type=int, default=300)\n",
    "    args.add_argument('--prob_dropout', type=float, default=0.4)\n",
    "    args.add_argument('--max_features', type=int, default=431)\n",
    "    \n",
    "    args.add_argument('--embedding', type=int, default=8)\n",
    "    args.add_argument('--threshold', type=float, default=0.5)\n",
    "    \n",
    "    config = args.parse_args()\n",
    "\n",
    "\n",
    "    if not HAS_DATASET and not IS_ON_NSML:  # It is not running on nsml\n",
    "        DATASET_PATH = '../sample_data/movie_review/'\n",
    "\n",
    "    # 모델의 specification\n",
    "    input_size = config.embedding*config.strmaxlen\n",
    "    output_size = 1\n",
    "    hidden_layer_size = 200\n",
    "    learning_rate = 0.001\n",
    "    character_size = 251\n",
    "    emb_train = False\n",
    "\n",
    "    def get_model(embedding_matrix, config):\n",
    "        inp = Input(shape=(config.maxlen, ), name='input')\n",
    "        x1 = Embedding(config.max_features, config.embed_size, weights=[embedding_matrix], trainable = emb_train)(inp)\n",
    "        x1 = SpatialDropout1D(config.prob_dropout)(x1)\n",
    "#         if cell_type_GRU:\n",
    "        x1 = Bidirectional(CuDNNLSTM(config.cell_size, return_sequences=True))(x1)\n",
    "        x1 = Bidirectional(CuDNNGRU(config.cell_size, return_sequences=True))(x1)\n",
    "#         else :\n",
    "#             x1 = Bidirectional(CuDNNLSTM(config.cell_size, return_sequences=True))(x1)\n",
    "#             x1 = Bidirectional(CuDNNLSTM(config.cell_size, return_sequences=True))(x1)\n",
    "        avg_pool1 = GlobalAveragePooling1D()(x1)\n",
    "        max_pool1 = GlobalMaxPooling1D()(x1)\n",
    "        ##merge\n",
    "        conc = concatenate([avg_pool1, max_pool1])\n",
    "        outp = Dense(output_size)(conc)\n",
    "        # DONOTCHANGE: Reserved for nsml\n",
    "        \n",
    "        model = Model(inputs=inp, outputs=outp)\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error', 'accuracy'])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "        \n",
    "\n",
    "    # DONOTCHANGE: Reserved for nsml\n",
    "    if config.pause:\n",
    "        nsml.paused(scope=locals())\n",
    "    \n",
    "    if config.mode == 'train':\n",
    "        # 데이터를 로드합니다.\n",
    "        dataset = MovieReviewDataset(DATASET_PATH, config.strmaxlen)\n",
    "        \n",
    "        ### csutom\n",
    "        dataset.dset_regex_morph(morph=True)\n",
    "        emb_path=\"D:/onedrive/code/ipython/wordvec/pre_trained/fasttext/ko/wiki.ko.vec\"\n",
    "        dataset.load_emb_model(embedding_path=emb_path)\n",
    "        dataset.dset_embedding(dataset.emb_model)\n",
    "        \n",
    "        mdl = get_model(dataset.emb_matrix, config)\n",
    "        nsml_callback = Nsml_Callback()\n",
    "        hist = mdl.fit(dataset.reviews, dataset.labels, batch_size=config.batch, epochs=config.epochs, callbacks = [nsml_callback], verbose=2)\n",
    "#             nsml.report(summary=True, scope=locals(), epoch=epoch, epoch_total=config.epochs,\n",
    "#                         train__loss=float(avg_loss/one_batch_size), step=epoch)\n",
    "            # DONOTCHANGE (You can decide how often you want to save the model)\n",
    "#             nsml.save(epoch)\n",
    "\n",
    "    # 로컬 테스트 모드일때 사용합니다\n",
    "    # 결과가 아래와 같이 나온다면, nsml submit을 통해서 제출할 수 있습니다.\n",
    "    # [(0.3, 0), (0.7, 1), ... ]\n",
    "    elif config.mode == 'test_local':\n",
    "        with open(os.path.join(DATASET_PATH, 'train/train_data'), 'rt', encoding='utf-8') as f:\n",
    "            queries = f.readlines()\n",
    "            \n",
    "        res = nsml.infer(queries)\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data is preprocessed\n",
      "=== Embedding Matrix is loaded\n",
      "Epoch 1/10\n",
      " - 2s - loss: 87.0811 - mean_squared_error: 87.0811 - acc: 0.0093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-03 01:51:26.073839: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n",
      "2018-04-03 01:51:26.357998: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1212] Found device 0 with properties: \n",
      "name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\n",
      "pciBusID: 0000:01:00.0\n",
      "totalMemory: 11.00GiB freeMemory: 9.09GiB\n",
      "2018-04-03 01:51:26.358366: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1312] Adding visible gpu devices: 0\n",
      "2018-04-03 01:51:26.929300: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8806 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "Traceback (most recent call last):\n",
      "  File \"trial.py\", line 157, in <module>\n",
      "    hist = mdl.fit(dataset.reviews, dataset.labels, batch_size=config.batch, epochs=config.epochs, callbacks = [nsml_callback], verbose=2)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1705, in fit\n",
      "    validation_steps=validation_steps)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1255, in _fit_loop\n",
      "    callbacks.on_epoch_end(epoch, epoch_logs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py\", line 77, in on_epoch_end\n",
      "    callback.on_epoch_end(epoch, logs)\n",
      "  File \"trial.py\", line 78, in on_epoch_end\n",
      "    bind_model(model=self.model)\n",
      "  File \"trial.py\", line 74, in bind_model\n",
      "    nsml.bind(save=save, load=load, infer=infer)\n",
      "NameError: name 'nsml' is not defined\n"
     ]
    }
   ],
   "source": [
    "!python trial.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "# -*- coding: utf-8 -*-\n",
    "def get_model_2rnn(\n",
    "                  embedding_matrix, cell_size = 80, cell_type_GRU = True,\n",
    "                  maxlen = 180, max_features = 100000, embed_size = 300,\n",
    "                  prob_dropout = 0.2, emb_train = False\n",
    "                 ):\n",
    "    \n",
    "    inp = Input(shape=(maxlen, ), name='input')\n",
    "\n",
    "    ##pre\n",
    "    x1 = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable = emb_train)(inp)\n",
    "    x1 = SpatialDropout1D(prob_dropout)(x1)\n",
    "    \n",
    "    if cell_type_GRU:\n",
    "        x1 = Bidirectional(CuDNNLSTM(cell_size, return_sequences=True))(x1)\n",
    "        x1 = Bidirectional(CuDNNGRU(cell_size, return_sequences=True))(x1)\n",
    "    else :\n",
    "        x1 = Bidirectional(CuDNNLSTM(cell_size, return_sequences=True))(x1)\n",
    "        x1 = Bidirectional(CuDNNLSTM(cell_size, return_sequences=True))(x1)\n",
    "    \n",
    "    avg_pool1 = GlobalAveragePooling1D()(x1)\n",
    "    max_pool1 = GlobalMaxPooling1D()(x1)\n",
    " \n",
    "    ##merge\n",
    "    conc = concatenate([avg_pool1, max_pool1])\n",
    "    outp = Dense(1)(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error', 'accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pipeline.py\n",
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "\n",
    "#===============keras ==============\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "#===============morphnizer ============\n",
    "from konlpy.tag import Twitter\n",
    "twt = Twitter()\n",
    "\n",
    "class MovieReviewDataset():\n",
    "    \"\"\"\n",
    "    영화리뷰 데이터를 읽어서, tuple (데이터, 레이블)의 형태로 리턴하는 파이썬 오브젝트 입니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_path: str, max_length: int):\n",
    "        \"\"\"\n",
    "        initializer\n",
    "        :param dataset_path: 데이터셋 root path\n",
    "        :param max_length: 문자열의 최대 길이\n",
    "        \"\"\"\n",
    "\n",
    "        # 데이터, 레이블 각각의 경로\n",
    "        data_review = os.path.join(dataset_path, 'train', 'train_data')\n",
    "        data_label = os.path.join(dataset_path, 'train', 'train_label')\n",
    "\n",
    "        # 영화리뷰 데이터를 읽고 preprocess까지 진행합니다\n",
    "        with open(data_review, 'rt', encoding='utf-8') as f:\n",
    "            self.reviews = f.readlines()\n",
    "        # 영화리뷰 레이블을 읽고 preprocess까지 진행합니다.\n",
    "        with open(data_label) as f:\n",
    "            self.labels = [np.float32(x) for x in f.readlines()]\n",
    "            \n",
    "        self.dset = pd.DataFrame(data=np.array([self.reviews, self.labels]).T, columns=['reviews', 'labels'])\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "\n",
    "        :return: 전체 데이터의 수를 리턴합니다\n",
    "        \"\"\"\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "\n",
    "        :param idx: 필요한 데이터의 인덱스\n",
    "        :return: 인덱스에 맞는 데이터, 레이블 pair를 리턴합니다\n",
    "        \"\"\"\n",
    "        return self.reviews[idx], self.labels[idx]\n",
    "    \n",
    "    \n",
    "\n",
    "    def load_emb_model(self, embedding_path, encodings = \"utf-8\"):\n",
    "        def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "        self.emb_model = dict(get_coefs(*o.strip().split(\" \")) for o in codecs.open(embedding_path, \"r\", encodings ))\n",
    "    \n",
    "    def dset_regex_morph(self, morph=True):\n",
    "        docs = []\n",
    "        for doc in self.dset['reviews']:\n",
    "            doc = re.sub('[^\\w?!]', ' ', doc)\n",
    "            doc = re.sub('[\\s]+', ' ', doc)\n",
    "            doc = re.sub('[\\s]$|^[\\s]', '', doc)\n",
    "            if morph:\n",
    "                docs.append(\" \".join(twt.morphs(doc)))\n",
    "            else:\n",
    "                docs.append(doc)\n",
    "        self.dset['reviews'] = docs\n",
    "    \n",
    "    def dset_embedding(self,\n",
    "                       emb_model,\n",
    "                       embed_size = 300,\n",
    "                       max_features = 100000,\n",
    "                       maxlen = 20,\n",
    "                       oov_zero = True,\n",
    "                       truncating='pre'\n",
    "                      ):\n",
    "        \n",
    "        doc_column = \"reviews\"\n",
    "        list_classes = [\"labels\"]\n",
    "\n",
    "        list_sentences = self.dset[doc_column].fillna('UNK').values.tolist()\n",
    "        \n",
    "\n",
    "        tokenizer = text.Tokenizer(num_words =max_features)\n",
    "        tokenizer.fit_on_texts(list_sentences)\n",
    "\n",
    "        list_tokenized = tokenizer.texts_to_sequences(list_sentences)\n",
    "        \n",
    "        X = sequence.pad_sequences(list_tokenized, maxlen=maxlen, truncating=truncating)\n",
    "        Y = self.dset[list_classes].values\n",
    "        print(\"=== Data is preprocessed\")\n",
    "\n",
    "        word_index = tokenizer.word_index\n",
    "        nb_words = min(max_features, len(word_index))\n",
    "\n",
    "        if oov_zero:\n",
    "            embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "        else:\n",
    "            embedding_matrix = np.random.normal(0.001, 0.4, (nb_words, embed_size))\n",
    "\n",
    "        for word, i in word_index.items():\n",
    "            if i >= max_features: continue\n",
    "            try:\n",
    "                embedding_vector = emb_model.get(word)\n",
    "                if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            except: \n",
    "                pass\n",
    "        print(\"=== Embedding Matrix is loaded\")\n",
    "        \n",
    "        self.reviews = X\n",
    "        self.labels = Y\n",
    "        self.emb_matrix = embedding_matrix\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data is preprocessed\n",
      "=== Embedding Matrix is loaded\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = '../sample_data/movie_review/'\n",
    "dataset = MovieReviewDataset(DATASET_PATH, 200)\n",
    "dataset.dset_regex_morph(morph=True)\n",
    "emb_path=\"D:/onedrive/code/ipython/wordvec/pre_trained/fasttext/ko/wiki.ko.vec\"\n",
    "dataset.load_emb_model(embedding_path=emb_path)\n",
    "emb_model = dataset.emb_model.copy()\n",
    "dataset.dset_embedding(emb_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
