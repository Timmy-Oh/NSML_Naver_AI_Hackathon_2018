{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import os\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.models.Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "        inp = Input(shape=(10, ), name='input')\n",
    "        x1 = Embedding(100, 30, trainable = False)(inp)\n",
    "#         x1 = Embedding(config.max_features, config.embed_size, weights=[embedding_matrix], trainable = emb_train)(inp)\n",
    "        x1 = SpatialDropout1D(0.4)(x1)\n",
    "#         if cell_type_GRU:\n",
    "        x1 = Bidirectional(CuDNNLSTM(40, return_sequences=True))(x1)\n",
    "        x1 = Bidirectional(CuDNNGRU(40, return_sequences=True))(x1)\n",
    "#         else :\n",
    "#             x1 = Bidirectional(CuDNNLSTM(config.cell_size, return_sequences=True))(x1)\n",
    "#             x1 = Bidirectional(CuDNNLSTM(config.cell_size, return_sequences=True))(x1)\n",
    "        avg_pool1 = GlobalAveragePooling1D()(x1)\n",
    "        max_pool1 = GlobalMaxPooling1D()(x1)\n",
    "        ##merge\n",
    "        conc = concatenate([avg_pool1, max_pool1])\n",
    "        outp = Dense(1)(conc)\n",
    "        # DONOTCHANGE: Reserved for nsml\n",
    "        \n",
    "        model = Model(inputs=inp, outputs=outp)\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error', 'accuracy'])\n",
    "        \n",
    "        return model\n",
    "mdl = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '1\\\\dataset.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-8a1414343d01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'dataset.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m             \u001b[0mpkl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1\\\\dataset.pkl'"
     ]
    }
   ],
   "source": [
    "with open(os.path.join('1', 'dataset.pkl'), 'wb') as f:\n",
    "            pkl.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nsml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "infer() missing 1 required positional argument: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-77bb959dbbd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnsml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: infer() missing 1 required positional argument: 'data'"
     ]
    }
   ],
   "source": [
    "nsml.infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\OneDrive\\\\GitHub\\\\ai-hackathon-2018\\\\missions\\\\examples\\\\movie-review\\\\src\\\\dataset.pkl'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.abspath('dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../starter/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../starter/main.py\n",
    "# -*- coding: utf-8 -*-\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "### custom\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "\n",
    "# keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input, concatenate, Flatten, add, multiply\n",
    "from keras.layers import CuDNNLSTM, CuDNNGRU, Bidirectional, Conv1D\n",
    "from keras.layers import Dropout, SpatialDropout1D, BatchNormalization, GlobalAveragePooling1D, GlobalMaxPooling1D, PReLU\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import K, Activation\n",
    "from keras.engine import Layer\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "### NSML\n",
    "import nsml\n",
    "from nsml import DATASET_PATH, HAS_DATASET, IS_ON_NSML\n",
    "from pipeline import MovieReviewDataset\n",
    "\n",
    "# IS_ON_NSML = False\n",
    "# HAS_DATASET = False\n",
    "\n",
    "# DONOTCHANGE: They are reserved for nsml\n",
    "# This is for nsml leaderboard\n",
    "\n",
    "def bind_model(model, dataset, config):\n",
    "    # 학습한 모델을 저장하는 함수입니다.\n",
    "    def save(filename, *args):\n",
    "        # directory\n",
    "        model.save_weights('{}_model.h5py'.format(filename))\n",
    "        print(\"{} is saved on nsml\".format(os.path.abspath('{}_model.h5py'.format(filename))))\n",
    "    \n",
    "        with open('{}_dataset.pkl'.format(filename), 'wb') as f:\n",
    "            pkl.dump(dataset, f)\n",
    "            print(\"{} is saved on nsml\".format(os.path.abspath('{}_dataset.pkl'.format(filename))))\n",
    "        \n",
    "\n",
    "    # 저장한 모델을 불러올 수 있는 함수입니다.\n",
    "    def load(filename, *args):\n",
    "        model.load_weights('{}_model.h5py'.format(filename))\n",
    "        print(\"{} is loaded\".format(os.path.abspath('{}_model.h5py'.format(filename))))\n",
    "        with open('{}_dataset.pkl'.format(filename), 'rb') as f:\n",
    "            dataset = pkl.load(f)\n",
    "        print(\"{} is loaded\".format(os.path.abspath('{}_dataset.pkl'.format(filename))))\n",
    "    \n",
    "    def infer(raw_data, **kwargs):\n",
    "        \"\"\"\n",
    "        :param raw_data: raw input (여기서는 문자열)을 입력받습니다\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # dataset.py에서 작성한 preprocess 함수를 호출하여, 문자열을 벡터로 변환합니다\n",
    "        preprocessed_data = dataset.preprocess(raw_data)\n",
    "        embedded_data = dataset.test_embedding(raw_data)\n",
    "\n",
    "        # 저장한 모델에 입력값을 넣고 prediction 결과를 리턴받습니다\n",
    "        pred = model.predict(embedded_data).squeeze(axis=1).tolist()\n",
    "        # DONOTCHANGE: They are reserved for nsml\n",
    "        return list(zip(np.zeros(len(pred)), pred))\n",
    "        \n",
    "    # DONOTCHANGE: They are reserved for nsml\n",
    "    # nsml에서 지정한 함수에 접근할 수 있도록 하는 함수입니다.\n",
    "    nsml.bind(save=save, load=load, infer=infer)\n",
    "    \n",
    "class Nsml_Callback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        nsml.report(summary=True, scope=locals(), epoch=epoch, epoch_total=config.epochs, step=epoch)\n",
    "        nsml.save(epoch)\n",
    "        \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args = argparse.ArgumentParser()\n",
    "    # DONOTCHANGE: They are reserved for nsml\n",
    "    args.add_argument('--mode', type=str, default='train')\n",
    "    args.add_argument('--pause', type=int, default=0)\n",
    "    args.add_argument('--iteration', type=str, default='0')\n",
    "\n",
    "    # User options\n",
    "    args.add_argument('--output', type=int, default=1)\n",
    "    args.add_argument('--epochs', type=int, default=5)\n",
    "    args.add_argument('--strmaxlen', type=int, default=400)\n",
    "    \n",
    "    args.add_argument('--maxlen', type=int, default=20)\n",
    "    args.add_argument('--cell_size', type=int, default=40)\n",
    "    args.add_argument('--embed_size', type=int, default=300)\n",
    "    args.add_argument('--prob_dropout', type=float, default=0.4)\n",
    "    args.add_argument('--max_features', type=int, default=431)\n",
    "    args.add_argument('--batch_size', type=int, default=256)\n",
    "    \n",
    "    config = args.parse_args()\n",
    "\n",
    "\n",
    "    if not HAS_DATASET and not IS_ON_NSML:  # It is not running on nsml\n",
    "        DATASET_PATH = '../sample_data/movie_review/'\n",
    "\n",
    "    # 모델의 specification\n",
    "    output_size = 1\n",
    "\n",
    "\n",
    "#     def get_model(embedding_matrix, config):\n",
    "    def get_model(config):\n",
    "        inp = Input(shape=(config.maxlen, ), name='input')\n",
    "        x1 = Embedding(config.max_features, config.embed_size, trainable = True)(inp)\n",
    "#         x1 = Embedding(config.max_features, config.embed_size, weights=[embedding_matrix], trainable = emb_train)(inp)\n",
    "        x1 = SpatialDropout1D(config.prob_dropout)(x1)\n",
    "#         if cell_type_GRU:\n",
    "        x1 = Bidirectional(CuDNNLSTM(config.cell_size, return_sequences=True))(x1)\n",
    "        x1 = Bidirectional(CuDNNGRU(config.cell_size, return_sequences=True))(x1)\n",
    "#         else :\n",
    "#             x1 = Bidirectional(CuDNNLSTM(config.cell_size, return_sequences=True))(x1)\n",
    "#             x1 = Bidirectional(CuDNNLSTM(config.cell_size, return_sequences=True))(x1)\n",
    "        avg_pool1 = GlobalAveragePooling1D()(x1)\n",
    "        max_pool1 = GlobalMaxPooling1D()(x1)\n",
    "        ##merge\n",
    "        conc = concatenate([avg_pool1, max_pool1])\n",
    "        outp = Dense(output_size)(conc)\n",
    "        \n",
    "        model = Model(inputs=inp, outputs=outp)\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error', 'accuracy'])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    mdl = get_model(config)\n",
    "    dataset = None\n",
    "    bind_model(model=mdl, dataset=dataset, config=config)\n",
    "\n",
    "    # DONOTCHANGE: Reserved for nsml\n",
    "    if config.pause:\n",
    "        nsml.paused(scope=locals())\n",
    "    \n",
    "    if config.mode == 'train':\n",
    "        # 데이터를 로드합니다.\n",
    "#         dataset = MovieReviewDataset(DATASET_PATH, config.strmaxlen)\n",
    "        #     dataset.export()\n",
    "#     dataset.prin()\n",
    "        dataset = MovieReviewDataset(DATASET_PATH, config.strmaxlen)\n",
    "        dataset.dset_regex_morph(morph=True)\n",
    "        dataset.dset_embedding()\n",
    "        ### csutom\n",
    "#         dataset.dset_regex_morph(morph=True)\n",
    "#         emb_path=\"D:/onedrive/code/ipython/wordvec/pre_trained/fasttext/ko/wiki.ko.vec\"\n",
    "#         dataset.load_emb_model(embedding_path=emb_path)\n",
    "#         dataset.dset_embedding(dataset.emb_model)\n",
    "#         dataset.dset_embedding()\n",
    "        \n",
    "#         mdl = get_model(dataset.emb_matrix, config)\n",
    "        \n",
    "        nsml_callback = Nsml_Callback()\n",
    "        hist = mdl.fit(dataset.reviews, dataset.labels, batch_size=config.batch_size, callbacks=[nsml_callback], epochs=config.epochs, verbose=2)\n",
    "        nsml.report(summary=True, scope=locals(), epoch=0, epoch_total=config.epochs, step=0)\n",
    "#         nsml.save(\"best\")\n",
    "            # DONOTCHANGE (You can decide how often you want to save the model)\n",
    "            \n",
    "\n",
    "    # 로컬 테스트 모드일때 사용합니다\n",
    "    # 결과가 아래와 같이 나온다면, nsml submit을 통해서 제출할 수 있습니다.\n",
    "    # [(0.3, 0), (0.7, 1), ... ]\n",
    "    elif config.mode == 'test_local':\n",
    "        with open(os.path.join(DATASET_PATH, 'train/train_data'), 'rt', encoding='utf-8') as f:\n",
    "            reviews = f.readlines()\n",
    "        res = nsml.infer(reviews)\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data is preprocessed\n",
      "Epoch 1/5\n",
      " - 6s - loss: 87.3536 - mean_squared_error: 87.3536 - acc: 0.0093\n",
      "Epoch 2/5\n",
      " - 0s - loss: 85.0647 - mean_squared_error: 85.0647 - acc: 0.0093\n",
      "Epoch 3/5\n",
      " - 0s - loss: 83.0166 - mean_squared_error: 83.0166 - acc: 0.0093\n",
      "Epoch 4/5\n",
      " - 0s - loss: 80.9355 - mean_squared_error: 80.9355 - acc: 0.0093\n",
      "Epoch 5/5\n",
      " - 0s - loss: 78.3273 - mean_squared_error: 78.3273 - acc: 0.0561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-03 22:01:00.450838: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n",
      "2018-04-03 22:01:00.898561: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1212] Found device 0 with properties: \n",
      "name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\n",
      "pciBusID: 0000:01:00.0\n",
      "totalMemory: 11.00GiB freeMemory: 9.09GiB\n",
      "2018-04-03 22:01:00.899254: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1312] Adding visible gpu devices: 0\n",
      "2018-04-03 22:01:03.573435: I C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8806 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "!python ../starter/main.py --mode train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "Traceback (most recent call last):\n",
      "  File \"../starter/main.py\", line 176, in <module>\n",
      "    res = nsml.infer(reviews)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nsml\\__init__.py\", line 34, in infer\n",
      "    return user_infer(data)\n",
      "  File \"../starter/main.py\", line 66, in infer\n",
      "    preprocessed_data = dataset.preprocess(raw_data)\n",
      "AttributeError: 'NoneType' object has no attribute 'preprocess'\n"
     ]
    }
   ],
   "source": [
    "!python ../starter/main.py --mode test_local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch :  1 / 1 , MSE in this minibatch:  19.29180335998535\n",
      "epoch: 0  train_loss: 19.29180335998535\n",
      "Batch :  1 / 1 , MSE in this minibatch:  7.607476711273193\n",
      "epoch: 1  train_loss: 7.607476711273193\n",
      "Batch :  1 / 1 , MSE in this minibatch:  7.607476711273193\n",
      "epoch: 2  train_loss: 7.607476711273193\n",
      "Batch :  1 / 1 , MSE in this minibatch:  7.607476711273193\n",
      "epoch: 3  train_loss: 7.607476711273193\n",
      "Batch :  1 / 1 , MSE in this minibatch:  7.607476711273193\n",
      "epoch: 4  train_loss: 7.607476711273193\n",
      "Batch :  1 / 1 , MSE in this minibatch:  7.607476711273193\n",
      "epoch: 5  train_loss: 7.607476711273193\n",
      "Batch :  1 / 1 , MSE in this minibatch:  7.607476711273193\n",
      "epoch: 6  train_loss: 7.607476711273193\n",
      "Batch :  1 / 1 , MSE in this minibatch:  7.607476711273193\n",
      "epoch: 7  train_loss: 7.607476711273193\n",
      "Batch :  1 / 1 , MSE in this minibatch:  7.607476711273193\n",
      "epoch: 8  train_loss: 7.607476711273193\n",
      "Batch :  1 / 1 , MSE in this minibatch:  7.607476711273193\n",
      "epoch: 9  train_loss: 7.607476711273193\n"
     ]
    }
   ],
   "source": [
    "!python ../example_back/main.py --mode train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 6.178287029266357), (0.0, 5.796324729919434), (0.0, 5.534942150115967), (0.0, 5.256916522979736), (0.0, 6.033804893493652), (0.0, 6.059906482696533), (0.0, 5.408660888671875), (0.0, 5.603948593139648), (0.0, 5.181177139282227), (0.0, 5.82639217376709), (0.0, 5.310539245605469), (0.0, 5.608646392822266), (0.0, 5.668211936950684), (0.0, 5.359910488128662), (0.0, 5.488841533660889), (0.0, 5.386354923248291), (0.0, 5.683938980102539), (0.0, 5.556982040405273), (0.0, 5.772186756134033), (0.0, 5.4370436668396), (0.0, 5.847136974334717), (0.0, 5.867918014526367), (0.0, 4.7786054611206055), (0.0, 5.748119831085205), (0.0, 5.2569990158081055), (0.0, 5.467550277709961), (0.0, 5.633947849273682), (0.0, 5.449449062347412), (0.0, 5.595730781555176), (0.0, 5.826915740966797), (0.0, 6.110252380371094), (0.0, 5.679713249206543), (0.0, 5.731297969818115), (0.0, 6.381442546844482), (0.0, 4.881044387817383), (0.0, 5.962987899780273), (0.0, 5.611995220184326), (0.0, 5.117802619934082), (0.0, 5.373388290405273), (0.0, 5.6925835609436035), (0.0, 5.821086883544922), (0.0, 5.639065742492676), (0.0, 5.32501220703125), (0.0, 5.389370918273926), (0.0, 5.506644248962402), (0.0, 5.646515846252441), (0.0, 5.614451885223389), (0.0, 5.8554606437683105), (0.0, 5.360039710998535), (0.0, 5.374545097351074), (0.0, 5.145179748535156), (0.0, 5.82164192199707), (0.0, 5.963075637817383), (0.0, 5.435402870178223), (0.0, 5.543752670288086), (0.0, 5.845027923583984), (0.0, 5.667875289916992), (0.0, 5.625134468078613), (0.0, 5.377874374389648), (0.0, 5.490350723266602), (0.0, 5.904119491577148), (0.0, 5.955855369567871), (0.0, 6.04366397857666), (0.0, 5.996642112731934), (0.0, 5.44520378112793), (0.0, 5.954058647155762), (0.0, 5.801898002624512), (0.0, 5.525479793548584), (0.0, 5.83826208114624), (0.0, 6.119445323944092), (0.0, 5.131963729858398), (0.0, 5.58213996887207), (0.0, 5.840641498565674), (0.0, 5.195883274078369), (0.0, 5.3643269538879395), (0.0, 5.472345352172852), (0.0, 5.749648571014404), (0.0, 5.25651741027832), (0.0, 5.483582019805908), (0.0, 5.464817523956299), (0.0, 5.787596225738525), (0.0, 5.768146991729736), (0.0, 5.761962413787842), (0.0, 5.679887771606445), (0.0, 6.3094987869262695), (0.0, 5.95349645614624), (0.0, 5.9102582931518555), (0.0, 5.460294246673584), (0.0, 5.258977890014648), (0.0, 5.956287384033203), (0.0, 5.371201515197754), (0.0, 6.1407246589660645), (0.0, 5.689437389373779), (0.0, 6.182966709136963), (0.0, 5.729982852935791), (0.0, 5.85650634765625), (0.0, 5.349749565124512), (0.0, 5.288824081420898), (0.0, 5.459272861480713), (0.0, 5.454100608825684), (0.0, 5.669361114501953), (0.0, 6.074279308319092), (0.0, 5.826407432556152), (0.0, 5.205722808837891), (0.0, 5.392374038696289), (0.0, 6.126417636871338), (0.0, 6.357702255249023)]\n"
     ]
    }
   ],
   "source": [
    "!python ../example_back/main.py --mode test_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../starter/pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../starter/pipeline.py\n",
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "\n",
    "#===============keras ==============\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "#===============morphnizer ============\n",
    "# from konlpy.tag import Twitter\n",
    "# twt = Twitter()\n",
    "\n",
    "class MovieReviewDataset():\n",
    "    \"\"\"\n",
    "    영화리뷰 데이터를 읽어서, tuple (데이터, 레이블)의 형태로 리턴하는 파이썬 오브젝트 입니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_path: str, max_length: int):\n",
    "        \"\"\"\n",
    "        initializer\n",
    "        :param dataset_path: 데이터셋 root path\n",
    "        :param max_length: 문자열의 최대 길이\n",
    "        \"\"\"\n",
    "\n",
    "        # 데이터, 레이블 각각의 경로\n",
    "        data_review = os.path.join(dataset_path, 'train', 'train_data')\n",
    "        data_label = os.path.join(dataset_path, 'train', 'train_label')\n",
    "\n",
    "        # 영화리뷰 데이터를 읽고 preprocess까지 진행합니다\n",
    "        with open(data_review, 'rt', encoding='utf-8') as f:\n",
    "            self.reviews = f.readlines()\n",
    "        # 영화리뷰 레이블을 읽고 preprocess까지 진행합니다.\n",
    "        with open(data_label) as f:\n",
    "            self.labels = [np.float32(x) for x in f.readlines()]\n",
    "            \n",
    "        self.dset = pd.DataFrame(data=np.array([self.reviews, self.labels]).T, columns=['reviews', 'labels'])\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "\n",
    "        :return: 전체 데이터의 수를 리턴합니다\n",
    "        \"\"\"\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "\n",
    "        :param idx: 필요한 데이터의 인덱스\n",
    "        :return: 인덱스에 맞는 데이터, 레이블 pair를 리턴합니다\n",
    "        \"\"\"\n",
    "        return self.reviews[idx], self.labels[idx]\n",
    "    \n",
    "    def export(self):\n",
    "        self.dset.to_csv(\"./export.csv\")\n",
    "        \n",
    "    def prin(self):\n",
    "        print(self.dset)   \n",
    "\n",
    "    def load_emb_model(self, embedding_path, encodings = \"utf-8\"):\n",
    "        def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "        self.emb_model = dict(get_coefs(*o.strip().split(\" \")) for o in codecs.open(embedding_path, \"r\", encodings ))\n",
    "    \n",
    "    def preprocess(self, raw_data, morph=True):\n",
    "        docs = []\n",
    "        for doc in raw_data:\n",
    "            doc = re.sub('[^\\w?!]', ' ', doc)\n",
    "            doc = re.sub('[\\s]+', ' ', doc)\n",
    "            doc = re.sub('[\\s]$|^[\\s]', '', doc)\n",
    "#             if morph:\n",
    "#                 docs.append(\" \".join(twt.morphs(doc)))\n",
    "#             else:\n",
    "#                 docs.append(doc)\n",
    "            docs.append(doc)\n",
    "        return docs\n",
    "    \n",
    "    def test_embedding(self, raw_data,\n",
    "#                        emb_model,\n",
    "                       embed_size = 300,\n",
    "                       max_features = 100000,\n",
    "                       maxlen = 20,\n",
    "                       oov_zero = True,\n",
    "                       truncating='pre'\n",
    "                      ):\n",
    "        \n",
    "\n",
    "        list_sentences = raw_data \n",
    "        list_tokenized = self.tokenizer.texts_to_sequences(list_sentences)\n",
    "        X = sequence.pad_sequences(list_tokenized, maxlen=maxlen, truncating=truncating)\n",
    "        return X\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def dset_regex_morph(self, morph=True):\n",
    "        docs = []\n",
    "        for doc in self.dset['reviews']:\n",
    "            doc = re.sub('[^\\w?!]', ' ', doc)\n",
    "            doc = re.sub('[\\s]+', ' ', doc)\n",
    "            doc = re.sub('[\\s]$|^[\\s]', '', doc)\n",
    "#             if morph:\n",
    "#                 docs.append(\" \".join(twt.morphs(doc)))\n",
    "#             else:\n",
    "#                 docs.append(doc)\n",
    "            docs.append(doc)\n",
    "        self.dset['reviews'] = docs\n",
    "    \n",
    "    def dset_embedding(self,\n",
    "#                        emb_model,\n",
    "                       embed_size = 300,\n",
    "                       max_features = 100000,\n",
    "                       maxlen = 20,\n",
    "                       oov_zero = True,\n",
    "                       truncating='pre'\n",
    "                      ):\n",
    "        \n",
    "        doc_column = \"reviews\"\n",
    "        list_classes = [\"labels\"]\n",
    "\n",
    "        list_sentences = self.dset[doc_column].fillna('UNK').values.tolist()\n",
    "        \n",
    "\n",
    "        self.tokenizer = text.Tokenizer(num_words =max_features)\n",
    "        self.tokenizer.fit_on_texts(list_sentences)\n",
    "\n",
    "        list_tokenized = self.tokenizer.texts_to_sequences(list_sentences)\n",
    "        \n",
    "        X = sequence.pad_sequences(list_tokenized, maxlen=maxlen, truncating=truncating)\n",
    "        Y = self.dset[list_classes].values\n",
    "        print(\"=== Data is preprocessed\")\n",
    "\n",
    "#         word_index = tokenizer.word_index\n",
    "#         nb_words = min(max_features, len(word_index))\n",
    "\n",
    "#         if oov_zero:\n",
    "#             embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "#         else:\n",
    "#             embedding_matrix = np.random.normal(0.001, 0.4, (nb_words, embed_size))\n",
    "\n",
    "#         for word, i in word_index.items():\n",
    "#             if i >= max_features: continue\n",
    "#             try:\n",
    "#                 embedding_vector = emb_model.get(word)\n",
    "#                 if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "#             except: \n",
    "#                 pass\n",
    "#         print(\"=== Embedding Matrix is loaded\")\n",
    "        \n",
    "        self.reviews = X\n",
    "        self.labels = Y\n",
    "#         self.emb_matrix = embedding_matrix\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
